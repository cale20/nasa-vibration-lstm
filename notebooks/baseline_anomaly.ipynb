{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f30b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# CELL 1: Imports & Global Configuration\n",
    "# --------------------------------------------------\n",
    "\"\"\"\n",
    "This cell establishes the global environment for our anomaly detection pipeline.\n",
    "\n",
    "Good ML systems are reproducible and configurable — NOT filled with magic numbers.\n",
    "Think of this as the control panel for the entire project.\n",
    "\n",
    "We are building infrastructure that will eventually support:\n",
    "    Isolation Forest  → baseline\n",
    "    Dense Autoencoder → next step\n",
    "    LSTM Autoencoder  → goal\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import seaborn as sns\n",
    "import joblib  # allows saving scalers/models for inference later\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Reproducibility (VERY IMPORTANT IN ML)\n",
    "# --------------------------------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Global Configuration\n",
    "# --------------------------------------------------\n",
    "CONFIG = {\n",
    "    \"sequence_length\": 100,       # size of sliding time window\n",
    "    \"stride\": 5,                  # how many timesteps to skip between sequences\n",
    "    \"batch_size\": 50,             # controls RAM usage during preprocessing\n",
    "    \"healthy_files\": 100,         # assumption: early machine life = healthy\n",
    "    \"num_files_to_process\": 400,  # subset of files for faster testing\n",
    "    \"processed_folder\": \"../data/processed\"  # where processed sequences/memmap go\n",
    "}\n",
    "\n",
    "sequence_length = CONFIG[\"sequence_length\"]\n",
    "batch_size = CONFIG[\"batch_size\"]\n",
    "stride = CONFIG[\"stride\"]\n",
    "processed_folder = CONFIG[\"processed_folder\"]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Paths\n",
    "# --------------------------------------------------\n",
    "data_folder = \"../data/raw/IMS/1st_test\"\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Global Scaler\n",
    "# --------------------------------------------------\n",
    "\"\"\"\n",
    "CRITICAL CONCEPT:\n",
    "\n",
    "We fit ONE scaler across healthy data so that\n",
    "\"normal vibration amplitude\" has a consistent definition.\n",
    "\n",
    "If you scaled each file independently:\n",
    "\n",
    "A failing machine would look statistically normal.\n",
    "\n",
    "This is one of the most common mistakes in anomaly detection.\n",
    "\"\"\"\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# TODOs\n",
    "# --------------------------------------------------\n",
    "# 1. Consider moving CONFIG to config.yaml for cleaner project structure.\n",
    "# 2. Save scaler after fitting: joblib.dump(scaler, 'scaler.pkl')\n",
    "# 3. Add GPU compatibility for future LSTM autoencoder training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52197806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 2: Sliding Window Sequence Generator\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "\n",
    "def create_sequences(signal, seq_length, stride=5):\n",
    "    \"\"\"\n",
    "    Converts a vibration signal into overlapping sequences.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : np.array, shape (timesteps, features)\n",
    "    seq_length : int\n",
    "        Number of timesteps per sequence\n",
    "    stride : int\n",
    "        How many timesteps to skip between sequences\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    sequences : np.array, shape (num_sequences, seq_length, features)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(signal) - seq_length, stride):\n",
    "        sequences.append(signal[i:i+seq_length])\n",
    "    return np.array(sequences, dtype=np.float32)\n",
    "\n",
    "# TODO: Later experiment with stride=1,2,5 to balance correlation and dataset size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 3: Discover & Filter IMS Vibration Files\n",
    "# -----------------------------\n",
    "import os\n",
    "\n",
    "def load_ims_files(folder, seq_length):\n",
    "    \"\"\"\n",
    "    Find all numeric IMS files and filter out files shorter than seq_length.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    valid_files : list[str]\n",
    "        Only files that can generate at least one sequence\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    for root, dirs, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            if f[-3] == \".\" and f[-2:].isdigit():\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    \n",
    "    all_files.sort()\n",
    "    valid_files = [f for f in all_files if len(np.loadtxt(f)) >= seq_length]\n",
    "\n",
    "    print(f\"Total files found: {len(all_files)}\")\n",
    "    print(f\"Files valid for processing: {len(valid_files)}\")\n",
    "    return valid_files\n",
    "\n",
    "files = load_ims_files(\"../data/raw/IMS/1st_test\", CONFIG[\"sequence_length\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86266eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# CELL 4 — Fit GLOBAL Scaler on Healthy Data\n",
    "# --------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "We assume early machine life is healthy.\n",
    "\n",
    "AUTOENCODERS REQUIRE THIS.\n",
    "\n",
    "They must learn what \"normal\" looks like.\n",
    "\"\"\"\n",
    "\n",
    "healthy_cutoff = CONFIG[\"healthy_files\"]\n",
    "sample_files = files[:healthy_cutoff]\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for file_path in sample_files:\n",
    "    signal = np.loadtxt(file_path, dtype=np.float32).reshape(-1,1)\n",
    "    all_samples.append(signal)\n",
    "\n",
    "all_samples = np.vstack(all_samples)\n",
    "\n",
    "scaler.fit(all_samples)\n",
    "\n",
    "print(\"✅ Global scaler fitted.\")\n",
    "print(\"Samples used:\", all_samples.shape)\n",
    "\n",
    "\n",
    "# Save scaler for inference\n",
    "joblib.dump(scaler, os.path.join(processed_folder, \"global_scaler.save\"))\n",
    "\n",
    "# TODO (EXTREMELY IMPORTANT):\n",
    "# NEVER train deep models on mixed healthy + failing data.\n",
    "#\n",
    "# Next step:\n",
    "# Save HEALTHY sequences separately for the autoencoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# CELL 5: Visualize Raw Signal\n",
    "# --------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Golden Rule of ML:\n",
    "\n",
    "NEVER trust data you haven't plotted.\n",
    "\"\"\"\n",
    "\n",
    "test_file = files[0]\n",
    "signal = np.loadtxt(test_file, dtype=np.float32)\n",
    "\n",
    "print(\"Signal shape:\", signal.shape)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(signal)\n",
    "plt.title(\"Healthy Machine Vibration Example\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# TODO (VERY HIGH VALUE):\n",
    "# Plot an EARLY file vs a LATE-LIFE file.\n",
    "#\n",
    "# When you SEE degradation:\n",
    "# modeling intuition skyrockets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d46e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 6: Create Memmap Dataset (Professional)\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "processed_folder = CONFIG[\"processed_folder\"]\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# Fit scaler on first N files (healthy assumption)\n",
    "scaler = StandardScaler()\n",
    "sample_files = files[:100]\n",
    "all_samples = []\n",
    "\n",
    "for f in sample_files:\n",
    "    signal = np.loadtxt(f).reshape(-1,1)\n",
    "    all_samples.append(signal)\n",
    "\n",
    "all_samples = np.vstack(all_samples)\n",
    "scaler.fit(all_samples)\n",
    "print(\"✅ Global scaler fitted | Samples used:\", all_samples.shape)\n",
    "\n",
    "# Estimate total sequences\n",
    "total_sequences = 0\n",
    "for f in files[:CONFIG[\"num_files_to_process\"]]:\n",
    "    signal = np.loadtxt(f).reshape(-1,1)\n",
    "    n_seq = (len(signal) - CONFIG[\"sequence_length\"]) // CONFIG[\"stride\"] + 1\n",
    "    if n_seq > 0:\n",
    "        total_sequences += n_seq\n",
    "\n",
    "print(\"Estimated total sequences:\", total_sequences)\n",
    "\n",
    "# Create memmap\n",
    "memmap_file = os.path.join(processed_folder, \"all_sequences.dat\")\n",
    "dataset_memmap = np.memmap(memmap_file, dtype='float32', mode='w+', shape=(total_sequences, CONFIG[\"sequence_length\"],1))\n",
    "\n",
    "# Write sequences into memmap\n",
    "write_idx = 0\n",
    "for f in files[:CONFIG[\"num_files_to_process\"]]:\n",
    "    signal = np.loadtxt(f).reshape(-1,1)\n",
    "    if len(signal) < CONFIG[\"sequence_length\"]:\n",
    "        print(f\"⚠️ Skipping {f} — too short for sequences\")\n",
    "        continue\n",
    "    scaled = scaler.transform(signal)\n",
    "    seqs = create_sequences(scaled, CONFIG[\"sequence_length\"], stride=CONFIG[\"stride\"])\n",
    "    dataset_memmap[write_idx:write_idx+len(seqs)] = seqs\n",
    "    write_idx += len(seqs)\n",
    "\n",
    "dataset_memmap.flush()\n",
    "print(\"✅ Memmap dataset created:\", memmap_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 7: Load Sequences Safely\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "\n",
    "X = np.memmap(memmap_file, dtype='float32', mode='r', shape=(total_sequences, CONFIG[\"sequence_length\"], 1))\n",
    "print(\"✅ Memmap loaded:\", X.shape)\n",
    "\n",
    "# Flatten for Isolation Forest\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "print(\"✅ Flattened shape for tree models:\", X_flat.shape)\n",
    "\n",
    "# Subset for fast training\n",
    "max_samples = 50000\n",
    "if X_flat.shape[0] > max_samples:\n",
    "    idx = np.random.choice(X_flat.shape[0], max_samples, replace=False)\n",
    "    X_flat_train = X_flat[idx]\n",
    "else:\n",
    "    X_flat_train = X_flat\n",
    "\n",
    "print(\"✅ Training on:\", X_flat_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eda287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# CELL 8: Train Baseline Anomaly Detector\n",
    "# --------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Senior ML Rule:\n",
    "\n",
    "Always beat a simple model before building a complex one.\n",
    "\"\"\"\n",
    "\n",
    "model = IsolationForest(\n",
    "    n_estimators=50,\n",
    "    contamination=0.01,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_flat_train)\n",
    "\n",
    "print(\"✅ Isolation Forest fit complete\")\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Plot score distribution.\n",
    "#\n",
    "# Understanding anomaly score shape\n",
    "# is a massively underrated ML skill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d22dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# CELL 9: Batch Scoring\n",
    "# --------------------------------------------------\n",
    "\n",
    "batch_size = 100000\n",
    "num_sequences = X_flat.shape[0]\n",
    "\n",
    "scores_list = []\n",
    "preds_list = []\n",
    "\n",
    "for start in range(0, num_sequences, batch_size):\n",
    "\n",
    "    end = min(start + batch_size, num_sequences)\n",
    "    X_batch = X_flat[start:end]\n",
    "\n",
    "    scores_list.append(model.decision_function(X_batch))\n",
    "    preds_list.append(model.predict(X_batch))\n",
    "\n",
    "scores = np.concatenate(scores_list)\n",
    "preds = np.concatenate(preds_list)\n",
    "\n",
    "np.save(os.path.join(processed_folder, \"anomaly_scores.npy\"), scores)\n",
    "\n",
    "print(\"✅ Batch scoring complete\")\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Save anomaly threshold.\n",
    "#\n",
    "# anomaly_threshold = np.percentile(scores, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3374414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 10: Robust Machine Health Curve\n",
    "# -----------------------------\n",
    "file_mean_scores = []\n",
    "sequence_counter = 0\n",
    "\n",
    "for f in files[:CONFIG[\"num_files_to_process\"]]:\n",
    "    signal = np.loadtxt(f).reshape(-1,1)\n",
    "    num_seqs = (len(signal) - CONFIG[\"sequence_length\"]) // CONFIG[\"stride\"] + 1\n",
    "\n",
    "    if num_seqs <= 0:\n",
    "        print(f\"⚠️ Skipping {f} — too short for sequences\")\n",
    "        continue\n",
    "\n",
    "    # Extract the slice of scores for this file\n",
    "    file_scores = scores[sequence_counter:sequence_counter + num_seqs]\n",
    "\n",
    "    if len(file_scores) == 0:\n",
    "        print(f\"⚠️ No scores found for {f}, skipping\")\n",
    "        continue\n",
    "\n",
    "    file_mean_scores.append(file_scores.mean())\n",
    "    sequence_counter += num_seqs\n",
    "\n",
    "# Remove accidental NaNs\n",
    "file_mean_scores = [s for s in file_mean_scores if not np.isnan(s)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(file_mean_scores, marker='o', markersize=3)\n",
    "plt.title(\"Machine Health Curve (Mean Anomaly Score per File)\")\n",
    "plt.xlabel(\"File Order (Time)\")\n",
    "plt.ylabel(\"Mean Anomaly Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Machine Health Curve plotted | files plotted:\", len(file_mean_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
